{"cells":[{"cell_type":"markdown","metadata":{"id":"n3J-_EWHh_A2"},"source":["## Data"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"RIom_SBDhAbl"},"outputs":[{"name":"stdout","output_type":"stream","text":["crimes_df data shape  (377282, 11)\n","fecha_hecho          0\n","fecha                0\n","codigo_barrio        0\n","year                 0\n","mes                  0\n","dia                  0\n","hora                 0\n","minuto               0\n","latitud          24239\n","longitud         24239\n","conducta             0\n","dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dayofyear</th>\n","      <th>week</th>\n","      <th>weekofyear</th>\n","      <th>dayofweek</th>\n","      <th>quarter</th>\n","      <th>weekday</th>\n","      <th>weekend</th>\n","      <th>fecha</th>\n","      <th>codigo_barrio</th>\n","      <th>year</th>\n","      <th>mes</th>\n","      <th>dia</th>\n","      <th>hora</th>\n","      <th>minuto</th>\n","      <th>latitud</th>\n","      <th>longitud</th>\n","      <th>conducta</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2/01/2017</td>\n","      <td>1006</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>30</td>\n","      <td>6.251426</td>\n","      <td>-75.570280</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2/01/2017</td>\n","      <td>519</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>6.274359</td>\n","      <td>-75.578320</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2/01/2017</td>\n","      <td>710</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>30</td>\n","      <td>6.283381</td>\n","      <td>-75.580012</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3/01/2017</td>\n","      <td>809</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>6.246295</td>\n","      <td>-75.552469</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3/01/2017</td>\n","      <td>1004</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>6.262067</td>\n","      <td>-75.570167</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3/01/2017</td>\n","      <td>1016</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>30</td>\n","      <td>6.247649</td>\n","      <td>-75.557575</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4/01/2017</td>\n","      <td>504</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>6.306256</td>\n","      <td>-75.566786</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>5/01/2017</td>\n","      <td>1107</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>21</td>\n","      <td>0</td>\n","      <td>6.247196</td>\n","      <td>-75.587879</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>5/01/2017</td>\n","      <td>1609</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>22</td>\n","      <td>30</td>\n","      <td>6.203295</td>\n","      <td>-75.598008</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>5/01/2017</td>\n","      <td>908</td>\n","      <td>2017</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>19</td>\n","      <td>30</td>\n","      <td>6.236215</td>\n","      <td>-75.549798</td>\n","      <td>HURTO_DE_CARRO</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   dayofyear  week  weekofyear  dayofweek  quarter  weekday  weekend  \\\n","0          2     1           1          0        1        0        0   \n","1          2     1           1          0        1        0        0   \n","2          2     1           1          0        1        0        0   \n","3          3     1           1          1        1        1        0   \n","4          3     1           1          1        1        1        0   \n","5          3     1           1          1        1        1        0   \n","6          4     1           1          2        1        2        0   \n","7          5     1           1          3        1        3        0   \n","8          5     1           1          3        1        3        0   \n","9          5     1           1          3        1        3        0   \n","\n","        fecha  codigo_barrio  year  mes  dia  hora  minuto   latitud  \\\n","0  2/01/2017            1006  2017    1    2    17      30  6.251426   \n","1  2/01/2017             519  2017    1    2    18       0  6.274359   \n","2  2/01/2017             710  2017    1    2    15      30  6.283381   \n","3  3/01/2017             809  2017    1    3     0      15  6.246295   \n","4  3/01/2017            1004  2017    1    3    15       0  6.262067   \n","5  3/01/2017            1016  2017    1    3     3      30  6.247649   \n","6  4/01/2017             504  2017    1    4    22       0  6.306256   \n","7  5/01/2017            1107  2017    1    5    21       0  6.247196   \n","8  5/01/2017            1609  2017    1    5    22      30  6.203295   \n","9  5/01/2017             908  2017    1    5    19      30  6.236215   \n","\n","    longitud        conducta  \n","0 -75.570280  HURTO_DE_CARRO  \n","1 -75.578320  HURTO_DE_CARRO  \n","2 -75.580012  HURTO_DE_CARRO  \n","3 -75.552469  HURTO_DE_CARRO  \n","4 -75.570167  HURTO_DE_CARRO  \n","5 -75.557575  HURTO_DE_CARRO  \n","6 -75.566786  HURTO_DE_CARRO  \n","7 -75.587879  HURTO_DE_CARRO  \n","8 -75.598008  HURTO_DE_CARRO  \n","9 -75.549798  HURTO_DE_CARRO  "]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import geopandas as gpd\n","from shapely.geometry import Point\n","import shapely.geometry\n","import pyproj\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import re\n","\n","\n","def clean_conducta(conducta):\n","    conducta = conducta.upper()\n","    conducta = conducta.replace(\" \", \"_\")\n","    conducta = re.sub(\n","        r\"[ÁÉÍÓÚ]\",\n","        lambda m: m.group(0)\n","        .replace(\"Á\", \"A\")\n","        .replace(\"É\", \"E\")\n","        .replace(\"Í\", \"I\")\n","        .replace(\"Ó\", \"O\")\n","        .replace(\"Ú\", \"U\"),\n","        conducta,\n","    )\n","    conducta = conducta.replace(\"Ñ\", \"N\")\n","    return conducta\n","\n","\n","crimes_df = pd.read_csv(\n","    \"./base_crim16_08.csv\"\n",")  # parse_dates=[\"fecha_hecho\"], delimiter=\";\")\n","\n","# Select relevant features\n","features = [\n","    \"fecha_hecho\",\n","    \"fecha\",\n","    \"codigo_barrio\",\n","    \"year\",\n","    \"mes\",\n","    \"dia\",\n","    \"hora\",\n","    \"minuto\",\n","    \"latitud\",\n","    \"longitud\",\n","    \"conducta\",\n","]\n","crimes_df = crimes_df[features]\n","\n","crimes_df[\"fecha_hecho\"] = pd.to_datetime(\n","    crimes_df[\"fecha_hecho\"], format=\"%d/%m/%Y %H:%M\"\n",")\n","\n","print(\"crimes_df data shape \", crimes_df.shape)\n","\n","# describe data and see if there are missing values\n","print(crimes_df.isnull().sum())  # check for missing values\n","crimes_df = crimes_df.dropna()  # drop missing values\n","crimes_df = crimes_df.drop_duplicates()  # elimianr los datos repetidos\n","# print(\"\\ncrimes dtypes: \", crimes_df.dtypes)\n","\n","\n","# ? DATE TIME STAMP FUNCTION\n","column_1 = crimes_df.iloc[:, 0]\n","\n","db = pd.DataFrame(\n","    {\n","        \"dayofyear\": column_1.dt.dayofyear,\n","        \"week\": column_1.dt.isocalendar().week,\n","        \"weekofyear\": column_1.dt.isocalendar().week,\n","        \"dayofweek\": column_1.dt.dayofweek,\n","        \"quarter\": column_1.dt.quarter,\n","        # ? kaggle\n","        \"weekday\": column_1.dt.weekday,\n","        \"weekend\": np.where(column_1.dt.weekday >= 4, 1, 0),\n","        # \"Season\": (\n","        #     column_1 - pd.DateOffset(months=1)\n","        # ).dt.quarter,\n","    }\n",")\n","\n","\n","crimes_df = crimes_df.drop(\"fecha_hecho\", axis=1)  #! drop fecha_hecho\n","crimes_df = pd.concat([db, crimes_df], axis=1)\n","# data = pd.get_dummies(crimes_df.conducta)\n","# crimes_df = pd.concat([data, crimes_df], axis=1)\n","crimes_df[\"conducta\"] = crimes_df[\"conducta\"].apply(clean_conducta)\n","\n","crimes_df = crimes_df[(crimes_df[\"year\"] >= 2008) & (crimes_df[\"year\"] <= 2018)]\n","\n","crimes_df[\"codigo_barrio\"] = crimes_df[\"codigo_barrio\"].astype(str).str.replace(\"#\", \"\")\n","\n","# parsear el dato de codigo_barrio a int, los que no se puedan parsear a interos se eliminan esos registros del dataframe\n","crimes_df[\"codigo_barrio\"] = pd.to_numeric(crimes_df[\"codigo_barrio\"], errors=\"coerce\")\n","\n","crimes_df.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["### SHP and intersections"]},{"cell_type":"markdown","metadata":{},"source":["#### Grids"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set up transformers, EPSG:3857 is metric, same as EPSG:900913\n","# to_proxy_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","# to_original_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","\n","\n","# # Definir la función de conversión de grados a metros\n","# def convertir_lat_long_a_metros(lat, long):\n","#     transformed = to_proxy_transformer.transform(lat, long)\n","#     return transformed[0], transformed[1]\n","\n","\n","# # Perform spatial join by rounding coordinates for more accurate intersection\n","# # def round_coords(geom):\n","# #     if geom.type == \"Polygon\":\n","# #         return shapely.geometry.Polygon(\n","# #             [(round(x, 4), round(y, 4)) for x, y in geom.exterior.coords]\n","# #         )\n","# #     elif geom.type == \"Point\":\n","# #         return Point(round(geom.x, 4), round(geom.y, 4))\n","# #     else:\n","# #         return geom\n","\n","\n","# def create_gdf(df):\n","#     gdf = df.copy()\n","#     gdf[\"Coordinates\"] = list(zip(gdf.longitud, gdf.latitud))\n","#     gdf.Coordinates = gdf.Coordinates.apply(Point)\n","#     gdf = gpd.GeoDataFrame(gdf, geometry=\"Coordinates\", crs=\"epsg:32618\")\n","#     return gdf\n","\n","\n","# # Leer el archivo SHP y crear un objeto GeoDataFrame\n","# ruta_archivo_shp = \"shp/BarrioVereda_2014.shp\"\n","# #ruta_archivo_shp = \"shp/map.shp\"\n","# gdf = gpd.read_file(ruta_archivo_shp)\n","\n","# # Filtrar por el código de comunas de Medellín (son 16 comunas en total)\n","# gdf = gdf[gdf[\"LIMITECOMU\"] < \"17\"]\n","\n","# # Reproyectar a coordenadas planas\n","# gdf = gdf.to_crs(epsg=32618)\n","\n","# print(gdf.columns.tolist()) #mostar los nombres de las columnas de la base\n","\n","# # Crear el polígono único que representa el límite exterior de todos los polígonos\n","# outter_map = gdf.unary_union\n","\n","# # Crear un nuevo GeoDataFrame con el polígono del límite exterior\n","# gdf_outter = gpd.GeoDataFrame(geometry=[outter_map], crs=gdf.crs)\n","\n","# xmin, ymin, xmax, ymax = gdf_outter.total_bounds\n","# print(\"xmin:\", xmin, \"ymin:\", ymin, \"xmax:\", xmax, \"ymax:\", ymax)\n","\n","# stepsize = 200  # 200 meters grid step size\n","\n","# # Iterate over 2D area and create grid squares (boxes) with consecutive grid_id\n","# gridboxes = []\n","# grid_id = 1  # Initialize grid_id to start from 1\n","# x = xmin\n","# while x < xmax:\n","#     y = ymin\n","#     while y < ymax:\n","#         box = shapely.geometry.box(x, y, x + stepsize, y + stepsize)\n","#         box_properties = {\"geometry\": box, \"grid_id\": grid_id}\n","#         gridboxes.append(box_properties)\n","#         grid_id += 1  # Increment grid_id for the next box\n","#         y += stepsize\n","#     x += stepsize\n","\n","# # Create a GeoDataFrame from gridboxes\n","# grid_gdf = gpd.GeoDataFrame(gridboxes, crs=gdf.crs)\n","# # grid_gdf[\"geometry\"] = grid_gdf[\"geometry\"].apply(round_coords)\n","\n","# print(grid_gdf.columns.tolist()) #mostar los nombres de las columnas de la base AQUI SE MUESTRA AREA\n","\n","# # Perform spatial join\n","# grid_outter_gdf = gpd.sjoin(grid_gdf, gdf_outter, how=\"inner\", op=\"within\")\n","# # grid_outter_gdf[\"geometry\"] = grid_outter_gdf[\"geometry\"].apply(round_coords)\n","\n","# print(grid_outter_gdf.columns.tolist()) #mostar los nombres de las columnas de la base AQUI SE MUESTRA AREA\n","\n","# # **** 2. Cargar los datos de los delitos ****\n","# crimes_gdf = create_gdf(crimes_df)\n","# print(len(crimes_gdf))\n","# # crimes_gdf[\"Coordinates\"] = crimes_gdf[\"Coordinates\"].apply(round_coords)\n","\n","# # Convertir las coordenadas de latitud y longitud a metros y crear una nueva columna\n","# crimes_gdf[\"Coordinates\"] = crimes_gdf.apply(\n","#     lambda row: Point(*convertir_lat_long_a_metros(row[\"latitud\"], row[\"longitud\"])),\n","#     axis=1,\n","# )\n","\n","\n","# # eliminar todos los coordinates que tengan infinity\n","# crimes_gdf = crimes_gdf[\n","#     crimes_gdf[\"Coordinates\"].apply(lambda x: not np.isinf(x.coords[0][0]))\n","# ]\n","\n","# # eliminar todos los registros que tengan latitud y longitud 0\n","# crimes_gdf = crimes_gdf[\n","#     crimes_gdf[\"Coordinates\"].apply(\n","#         lambda x: x.coords[0][0] != 0 and x.coords[0][1] != 0\n","#     )\n","# ]\n","\n","# # Realizar la intersección usando gpd.sjoin()\n","# crimes_gdf = gpd.sjoin(\n","#     crimes_gdf, grid_outter_gdf[[\"grid_id\", \"geometry\"]], how=\"inner\", op=\"within\"\n","# )\n","# print(len(crimes_gdf))\n","\n","# # eliminar columna de index_right\n","# crimes_gdf = crimes_gdf.drop(columns=[\"index_right\"])\n","\n","# print(crimes_gdf.head())\n","\n","# print(\"\\n --------------------\")\n","\n","# print(grid_outter_gdf.head())\n","\n","# # Plot the map and grid squares\n","# fig, ax = plt.subplots(figsize=(10, 8))\n","# grid_outter_gdf.plot(ax=ax, color=\"white\", edgecolor=\"black\")\n","# crimes_gdf.plot(ax=ax, color=\"red\", markersize=1)\n","# # gdf_outter.plot(ax=ax, color='lightgray', edgecolor='black')\n","\n","\n","# plt.title(\"Mapa con cuadrícula de cuadrados de 200 metros\")\n","# plt.xlabel(\"Longitud (EPSG:32618)\")\n","# plt.ylabel(\"Latitud (EPSG:32618)\")\n","# plt.show()\n","#print(gdf.columns.tolist()) #para ver el nombre de las columnas de una base de datos"]},{"cell_type":"markdown","metadata":{},"source":["### Barrios"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    CODIGO                  NOMBRE  ABREVIATUR IDENTIFICA LIMITECOMU  \\\n","0  Inst_15                   U.P.B         NaN    Inst_15         11   \n","1  Inst_18          Cerro Nutibara         NaN    Inst_18         16   \n","2     0610        Mirador del Doce         NaN       0610         06   \n","3     1620  El Nogal-Los Almendros         NaN       1620         16   \n","4     0612              El Triunfo         NaN       0612         06   \n","\n","   ZHFISICAGE LIMITEMUNI  VIGENCIA_F  SUBTIPO_BA  LINK_DOCUM           area  \\\n","0         NaN        001         NaN         1.0         NaN  216275.239275   \n","1         NaN        001         NaN         1.0         NaN  398713.874885   \n","2         NaN        001         NaN         1.0         NaN   64749.995150   \n","3         NaN        001         NaN         1.0         NaN  248088.656801   \n","4         NaN        001         NaN         1.0         NaN  116938.310530   \n","\n","                                            geometry  \n","0  POLYGON ((434831.540 690264.857, 434844.278 69...  \n","1  POLYGON ((436204.610 689474.103, 436249.813 68...  \n","2  POLYGON ((435349.637 696977.914, 435355.180 69...  \n","3  POLYGON ((434122.531 689397.894, 434121.530 68...  \n","4  POLYGON ((435164.198 697516.984, 435166.244 69...  \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nicolas\\Documents\\TRABAJOS\\TESIS_CAMILA\\workspace_python\\myvenv39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n","  if await self.run_code(code, result, async_=asy):\n"]},{"name":"stdout","output_type":"stream","text":["\n"," ---------------------------------------------------------- \n","\n","        dayofyear  week  weekofyear  dayofweek  quarter  weekday  weekend  \\\n","73816          87    13          13          0        1        0        0   \n","87612         182    27          27          1        3        1        0   \n","19922         219    32          32          2        3        2        0   \n","98361         229    34          34          0        3        0        0   \n","333216        124    18          18          6        2        6        1   \n","\n","             fecha  codigo_barrio  year  mes  dia  hora  minuto   latitud  \\\n","73816   28/03/2011           1001  2011    3   28     0       5  6.263111   \n","87612   1/07/2014             101  2014    7    1    18      30  6.295385   \n","19922   6/08/2008            1108  2008    8    6    19       0  6.238979   \n","98361   17/08/2015           1006  2015    8   17     6       0  6.253477   \n","333216  4/05/2014             803  2014    5    4     4      27  6.255785   \n","\n","         longitud         conducta                    Coordinates  \\\n","73816  -75.563112  HURTO_A_PERSONA  POINT (437710.260 692322.965)   \n","87612  -75.543119  HURTO_A_PERSONA  POINT (439925.591 695888.469)   \n","19922  -75.596610   HURTO_DE_CARRO  POINT (434001.660 689659.286)   \n","98361  -75.572202  HURTO_A_PERSONA  POINT (436703.510 691259.078)   \n","333216 -75.558879        HOMICIDIO  POINT (438177.655 691512.587)   \n","\n","                 area  \n","73816   623704.803588  \n","87612   560584.104373  \n","19922   706189.603692  \n","98361   492771.466673  \n","333216  371127.351803  \n"]}],"source":["# Set up transformers, EPSG:3857 is metric, same as EPSG:900913\n","to_proxy_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","to_original_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","\n","# Definir la función de conversión de grados a metros\n","def convertir_lat_long_a_metros(lat, long):\n","    transformed = to_proxy_transformer.transform(lat, long)\n","    return transformed[0], transformed[1]\n","\n","def create_gdf(df):\n","    gdf = df.copy()\n","    gdf[\"Coordinates\"] = list(zip(gdf.longitud, gdf.latitud))\n","    gdf.Coordinates = gdf.Coordinates.apply(Point)\n","    gdf = gpd.GeoDataFrame(gdf, geometry=\"Coordinates\", crs=\"epsg:32618\")\n","    return gdf\n","\n","crimes_gdf = create_gdf(crimes_df)\n","# print(\"len crimes_gdf ->\",len(crimes_gdf))\n","\n","# Convertir las coordenadas de latitud y longitud a metros y crear una nueva columna\n","crimes_gdf[\"Coordinates\"] = crimes_gdf.apply(\n","    lambda row: Point(*convertir_lat_long_a_metros(row[\"latitud\"], row[\"longitud\"])),\n","    axis=1,\n",")\n","\n","# eliminar todos los coordinates que tengan infinity\n","crimes_gdf = crimes_gdf[\n","    crimes_gdf[\"Coordinates\"].apply(lambda x: not np.isinf(x.coords[0][0]))\n","]\n","\n","# eliminar todos los registros que tengan latitud y longitud 0\n","crimes_gdf = crimes_gdf[\n","    crimes_gdf[\"Coordinates\"].apply(\n","        lambda x: x.coords[0][0] != 0 and x.coords[0][1] != 0\n","    )\n","]\n","\n","# ***************** Leer el archivo SHP y crear un objeto GeoDataFrame ***************\n","ruta_shp = \"shp/map.shp\"\n","gdf = gpd.read_file(ruta_shp)\n","\n","# Filtrar por el código de comunas de Medellín (son 16 comunas en total)\n","gdf = gdf[gdf[\"LIMITECOMU\"] < \"17\"]\n","\n","# Reproyectar a coordenadas planas\n","gdf = gdf.to_crs(epsg=32618)\n","\n","# imprimir algunas filas del archivo shp\n","print(gdf.head(5))\n","\n","# Realizar la intersección usando gpd.sjoin()\n","crimes_gdf = gpd.sjoin(\n","    crimes_gdf, gdf[[\"area\",\"geometry\"]], how=\"inner\", op=\"within\"\n",")\n","\n","# eliminar columna de index_right\n","crimes_gdf = crimes_gdf.drop(columns=[\"index_right\"])\n","\n","print(\"\\n ---------------------------------------------------------- \\n\")\n","# imprimer algunos datos aleatorios del dataframe crimes_gdf\n","print(crimes_gdf.sample(5))"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fecha</th>\n","      <th>codigo_barrio</th>\n","      <th>conducta</th>\n","      <th>crime_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3426605</th>\n","      <td>2015-09-07</td>\n","      <td>724</td>\n","      <td>EXTORSION</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2332467</th>\n","      <td>2013-03-25</td>\n","      <td>608</td>\n","      <td>EXTORSION</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3523723</th>\n","      <td>2015-11-25</td>\n","      <td>1605</td>\n","      <td>HURTO_A_PERSONA</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3858476</th>\n","      <td>2016-08-26</td>\n","      <td>301</td>\n","      <td>HURTO_DE_CARRO</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3623367</th>\n","      <td>2016-02-15</td>\n","      <td>1001</td>\n","      <td>HURTO_DE_CARRO</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             fecha  codigo_barrio         conducta  crime_count\n","3426605 2015-09-07            724        EXTORSION          0.0\n","2332467 2013-03-25            608        EXTORSION          0.0\n","3523723 2015-11-25           1605  HURTO_A_PERSONA          0.0\n","3858476 2016-08-26            301   HURTO_DE_CARRO          0.0\n","3623367 2016-02-15           1001   HURTO_DE_CARRO          0.0"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# Eliminar espacios al principio y al final de los valores\n","crimes_gdf[\"fecha\"] = crimes_gdf[\"fecha\"].str.strip()\n","\n","# Convertir a datetime\n","crimes_gdf[\"fecha\"] = pd.to_datetime(crimes_gdf[\"fecha\"], format=\"%d/%m/%Y\")\n","\n","# Agrupar los datos por 'codigo_barrio', 'fecha' y 'conducta' y contar las ocurrencias\n","crime_count_per_day = crimes_gdf.groupby(\n","    [\"codigo_barrio\", \"fecha\", \"conducta\"]\n",").size()  # ,'year'\n","\n","# Desapilar y rellenar los datos faltantes con 0\n","crime_count_per_day = (\n","    crime_count_per_day.unstack([\"codigo_barrio\", \"conducta\"])\n","    .asfreq(\"D\")\n","    .fillna(0)\n","    .stack([\"codigo_barrio\", \"conducta\"])\n","    .reset_index(name=\"crime_count\")\n",")\n","\n","# Renombrar la columna de conteo\n","crime_count_per_day = pd.DataFrame(crime_count_per_day)\n","\n","crime_count_per_day.sample(5)\n","\n","# crime_count_per_day.to_csv('crimecount.csv')"]},{"cell_type":"markdown","metadata":{},"source":["### Load and Merge characteristics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["characteristics = pd.read_excel(\"./characteristics_edad.xlsx\")\n","\n","characteristics = characteristics.drop(\n","    [\n","        \"rentista\",\n","        \"viv_desechos\",\n","        \"otra_acti\",\n","        \"union_libre\",\n","        \"apto\",\n","        \"casa\",\n","        \"especial\",\n","        \"cuartos\",\n","        \"bloque\",\n","        \"divor\",\n","        \"viudo\",\n","        \"mate_pre\",\n","        \"inca_work\",\n","        \"ladrillo\",\n","        \"si_afili_pen\",\n","        \"si_arl\",\n","        \"cemento\",\n","        \"madera_burda\",\n","        \"mate_desechos\",\n","        \"baldosa\",\n","        \"tierra_arena\",\n","        \"si_estu\",\n","        \"madera_b\",\n","    ],\n","    axis=1,\n",")\n","# eliminaar registros que tengan CODIGO null nan o con un caracter '.'\n","characteristics = characteristics[characteristics[\"CODIGO\"].notna()]\n","characteristics = characteristics[characteristics[\"CODIGO\"] != \".\"]\n","\n","# convertir a entero el codigo\n","characteristics[\"CODIGO\"] = characteristics[\"CODIGO\"].astype(int)\n","\n","# renombrar columna de characteristics \"CODIGO\" a \"codigo_barrio\"\n","characteristics = characteristics.rename(columns={\"CODIGO\": \"codigo_barrio\"})\n","\n","# Se realiza el merge de crime_count_per_day characteristics left de crime_count_per_day\n","crime_count_per_day['year'] = pd.to_datetime(crime_count_per_day['fecha']).dt.year\n","crime_count_per_day = crime_count_per_day.merge(characteristics, how='left', on=['codigo_barrio', 'year'])\n","crime_count_per_day.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Description of our new dataset \n","crime_count_per_day.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Missing values: \")\n","print(crime_count_per_day.isnull().sum())\n","# drop missing values\n","print(\"dropping missing values\")\n","crime_count_per_day = crime_count_per_day.dropna()\n","# elimianr los datos repetidos\n","print(\"eliminar los datos repetidos\")\n","crime_count_per_day = crime_count_per_day.drop_duplicates()\n","\n","#crime_count_per_day.to_csv('crimecount_merge.csv')\n","crime_count_per_day.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crime_count_per_day.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Instances of Crime_count per date, neighborhood, and crime category\n","cc = crime_count_per_day.groupby('Crime_count').size()\n","fig, axes = plt.subplots(1, 2, figsize=(14, 4));\n","cc.plot.bar(logy=False, ax=axes[0], title='Instances of Crime_count, lin axes');\n","cc.plot.bar(logy=True, ax=axes[1], title='Instances of Crime_count, log-lin axes');"]},{"cell_type":"markdown","metadata":{},"source":["### Correlation features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pip install seaborn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import seaborn as sns\n","# sns.set()\n","# #Adding dummy feature to get full cmap scale (really complicated to solve in sns/matplotlib)\n","# crime_count_per_day['dummy_feature'] = -crime_count_per_day['pensionado']\n","# corr = crime_count_per_day.corr()\n","\n","# # Create a heatmap\n","# fig, ax = plt.subplots(figsize=(10, 8))\n","# sns.heatmap(corr,\n","#             xticklabels=corr.columns.values,\n","#             yticklabels=corr.columns.values,\n","#             cmap='RdBu_r',\n","#             ax=ax)\n","\n","# # Remove the dummy feature\n","# crime_count_per_day = crime_count_per_day.drop('dummy_feature', axis=1)\n","\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Train-dev-test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort columns on name for easier handling of predictions downstream\n","crime_data_grd = crime_count_per_day.reindex(\n","    sorted(crime_count_per_day.columns), axis=1\n",")\n","\n","X_train = crime_data_grd[crime_data_grd['year'] <= 2017] \n","#crime_data_grd.query('fecha < \"2018-01-01\"')\n","X_test = crime_data_grd[crime_data_grd['year'] == 2018]  \n","#crime_data_grd.query('fecha >= \"2018-01-01\"')\n","\n","#labels for classifier\n","y_train_clf = X_train['Crime_count'].clip(upper=1).astype(int)\n","y_test_clf = X_test['Crime_count'].clip(upper=1).astype(int)\n","\n","#labels for regression\n","y_train_reg = X_train['Crime_count']\n","y_test_reg = X_test['Crime_count']\n","\n","#Clean\n","X_train = X_train.drop(['Crime_count'],axis=1)\n","X_test = X_test.drop(['Crime_count'],axis=1)\n","\n","#Check split ratios\n","time_split = int(len(X_train)/(len(crime_data_grd))*100)\n","event_split = int(y_train_clf.sum()/(y_train_clf.sum()+y_test_clf.sum())*100)\n","print(\"Train/test split in time: {}/{}\".format(time_split, 100-time_split))\n","print(\"Train/test split in events: {}/{}\".format(event_split, 100-event_split))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Model preprocessing pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install sklearn-pandas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler, LabelBinarizer\n","from sklearn_pandas import DataFrameMapper, gen_features\n","from sklearn.pipeline import FeatureUnion\n","\n","#source: https://github.com/ageron/handson-ml\n","class LabelBinarizerPipelineFriendly(LabelBinarizer):\n","    \"\"\"Binarize Label in a One vs all fashion. This utility class would\n","    enable people to use this with categorical variables in pipelines.\n","    Refer to LabelBinarizer for detailed info.\n","    \"\"\"\n","\n","    def fit(self, X, y=None):\n","        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n","        super(LabelBinarizerPipelineFriendly, self).fit(X)\n","\n","    def transform(self, X, y=None):\n","        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n","\n","    def fit_transform(self, X, y=None):\n","        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Numerical features. Using a StandardScaler to be insensitive to any outliers.\n","num_attributes = X_train.select_dtypes(exclude=[object]).columns\n","num_mapper = DataFrameMapper(gen_features(\n","    columns=[[f] for f in num_attributes],\n","    classes=[\n","            StandardScaler\n","            ]\n","))\n","\n","#Categorical features. One-hot encode (binarize) the features.\n","cat_attributes = X_train.select_dtypes(include=[object]).columns\n","cat_mapper = DataFrameMapper(gen_features(\n","    columns=[[f] for f in cat_attributes],\n","    classes=[LabelBinarizerPipelineFriendly]\n","))\n","\n","#Union\n","full_pipeline = FeatureUnion(transformer_list=[\n","    ('num_mapper', num_mapper),\n","    ('cat_mapper', cat_mapper),\n","])\n","\n","X_train_prep = full_pipeline.fit_transform(X_train)\n","X_test_prep = full_pipeline.transform(X_test)\n","print(num_attributes)\n","print(cat_attributes)\n","print(np.shape(X_train_prep))\n","print(X_train_prep[1])\n"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter grid search"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import randint as sp_randint\n","from sklearn.metrics import precision_recall_curve, confusion_matrix, make_scorer, classification_report\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_predict, RandomizedSearchCV\n","# def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n","# def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n","# def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n","# def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n","# def get_standard_scores():\n","#     scores = {'recall' : 'recall', 'precision' : 'precision',\n","#     'roc_auc' : 'roc_auc', 'avg_precision' : 'average_precision',\n","#     'f1' : 'f1', \n","#     'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n","#     'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n","#     return scores\n","# #Uncomment the three bottom lines in the cell and it'll search and print the results.\n","# param_grid = {\n","#     'max_depth': sp_randint(5, 80),\n","#     'n_estimators' : sp_randint(1, 200),\n","#     'min_samples_leaf' : sp_randint(1, 400),\n","#     'random_state' : [42],\n","#     'max_features' : ['auto'],\n","#     'class_weight' : ['balanced', None]\n","#     }\n","# def generate_results_df(grid_searcher, scores, generate_files=False):\n","#     #keep all columns for future analyses\n","#     result_df_full = pd.DataFrame(grid_searcher.cv_results_)\n","#     #create a nimbler df\n","#     result_df_compact = pd.DataFrame(index=result_df_full.index)\n","#     for col in [col for col in result_df_full.columns if 'mean_test' in col]:\n","#         #First, check to see if we have a mean_test_score column, whcih indicates only one unnamed metric (from BayesSearchCV)\n","#         #If so, fill in name; else, handle multiple metrics. We don't have to break the loop, since mean_test_score implies only one 'mean_test'-column.\n","#         if 'mean_test_score' in col and type(scores) is str:\n","#             result_df_compact[scores] = result_df_full[[col, col.replace('mean_', 'std_')]].round(3).astype(str).apply(lambda x: ' +/- '.join(x), axis=1)\n","#         else:\n","#             #Confusion matrix must be handled separately\n","#             if '_tp' not in col and '_tn' not in col and '_fn' not in col and '_fp' not in col:\n","#                 result_df_compact[col.replace('mean_test_','')] = result_df_full[[col, col.replace('mean_', 'std_')]].round(3).astype(str).apply(lambda x: ' +/- '.join(x), axis=1)\n","#             else:\n","#                 #only do the confusion once\n","#                 if 'tp_fp_fn_tn' not in result_df_compact.columns:\n","#                     for metric in ['tp','fp','fn','tn']:\n","#                         #sum confusion matrix elements to match total number of samples\n","#                         result_df_full[metric+'_sum'] = result_df_full[[column for column in result_df_full.columns if metric in column and 'split' in column]].sum(axis=1)\n","#                     result_df_compact['tp_fp_fn_tn'] = result_df_full[['tp_sum', 'fp_sum', 'fn_sum', 'tn_sum']].astype(int).astype(str).apply(lambda x: '/'.join(x), axis=1)\n","#     result_df_compact['time_fit_score'] = result_df_full[['mean_fit_time', 'mean_score_time']].round(3).astype(str).apply(lambda x: ' + '.join(x), axis=1)\n","#     #add all parameters individually to the compact df\n","#     param_cols=[col for col in result_df_full.columns if 'param_' in col]\n","#     #transfer all parameter columns to compact df\n","#     for col in param_cols:\n","#         result_df_compact[col] = result_df_full[col].copy()\n","#     #we usually want to save the results in a file\n","#     if generate_files:\n","#         estimator=str(grid_searcher.estimator).split('(')[0]+'_'\n","#         #If we use a pipeline, list all estimators/samplers/tranformers in the file name\n","#         if estimator=='Pipeline_':\n","#             estimator=\"\"\n","#             for key, value in grid_searcher.estimator.named_steps.items():\n","#                 estimator+=str(value).split('(')[0]+'_'\n","#         #TODO: set output dir of file\n","#         opwd = './'\n","#         ofile = 'Results_df_'+estimator+time.strftime(\"%Y%m%d_%H%M%S\")+'.csv'\n","#         result_df_compact.to_csv(opwd+ofile, sep=',')\n","#         ofile = 'Full_results_df_'+estimator+'_'+time.strftime(\"%Y%m%d_%H%M%S\")+'.csv'\n","#         result_df_full.to_csv(opwd+ofile, sep=',')\n","#     return result_df_full, result_df_compact\n","\n","# scores = get_standard_scores()\n","\n","# n_iter = 10; cv=3\n","# rfc = RandomForestClassifier()\n","# rnd_search = RandomizedSearchCV(rfc, param_grid, n_iter=n_iter, cv=cv, scoring=scores, n_jobs=-1, refit=False, return_train_score=False, verbose=2)\n","# rnd_search.fit(X_train_prep, y_train_clf)\n","\n","# _, gs_df = generate_results_df(rnd_search, scores)\n","# gs_df.sort_values(by='avg_precision', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Select best models: best balanced classes and unbalanced classes. \n","#Could use refit=True in search above if you want to include search and run code top-to-bottom.\n","rfc1 = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=None, max_depth=77, n_estimators=81, min_samples_leaf=152)\n","rfc1.fit(X_train_prep, y_train_clf)\n","#rfc2 = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced', max_depth=69, n_estimators=64, min_samples_leaf=132)\n","#rfc2.fit(X_train_prep, y_train_clf)"]},{"cell_type":"markdown","metadata":{},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def obtain_and_plot_evaluation_figures(model, X, y, method, datatype, cv=3, verbose=100, title_prefix=None):\n","    if datatype == 'train':\n","        y_pred = cross_val_predict(model, X, y, cv=cv, method=method, verbose=verbose)\n","    elif datatype == 'test':\n","        y_pred = model.predict_proba(X)\n","    precisions, recalls, thresholds = precision_recall_curve(y, y_pred[:,1])\n","        #y_pred = model.predict_proba(X)\n","        \n","    # if datatype == 'train':\n","    #     precisions, recalls, thresholds = precision_recall_curve(y, y_pred)\n","    # else:\n","    #     precisions, recalls, thresholds = precision_recall_curve(y, y_pred)\n","\n","    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5));\n","    ax1.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\");\n","    ax1.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\");\n","    ax1.set(xlabel=\"Thresholds\");\n","    ax1.set_ylim([0,1]);\n","    ax1.legend();\n","    ax1.set_title(title_prefix + 'Precision & recall at thresholds');\n","\n","    ax2.plot(precisions, recalls, label=None);\n","    ax2.set_ylim([0,1]);\n","    ax2.set_xlim([0,1]);\n","    ax2.set(xlabel=\"Precision\", ylabel=\"Recall\");\n","    ax2.set_title(title_prefix + 'Precision-recall curve');\n","    print(np.shape(y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install joblib\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import joblib\n","# Guardar el modelo en un archivo con compresión\n","model_filename = 'random_forest_model_compre_16_08.joblib'\n","joblib.dump(rfc1, model_filename, compress=5)  # El valor 3 controla el nivel de compresión\n","# Cargar el modelo desde el archivo\n","loaded_model = joblib.load(model_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(y_train_clf.head())\n","\n","print(y_train_clf.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["obtain_and_plot_evaluation_figures(model=rfc1, X=X_train_prep, y=y_train_clf, datatype='train', method='predict_proba', cv=3, verbose=0, title_prefix='CV:ed TRAINING DATA: ')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["obtain_and_plot_evaluation_figures(model=rfc1, X=X_test_prep, y=y_test_clf, datatype='test', method='predict_proba', verbose=0, title_prefix='TEST DATA: ')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#print('CV:ed TRAINING DATA: \\n' + classification_report(y_train_clf, cross_val_predict(rfc1, X_train_prep, y_train_clf, cv=3)))\n","print('TEST DATA: \\n' + classification_report(y_test_clf, rfc1.predict(X_test_prep)))"]},{"cell_type":"markdown","metadata":{},"source":["### Duplicated train/test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#All duplicates and also without alpha-days crime density columns\n","#Training data\n","dupl = X_train.duplicated().sum()\n","dupl_wo_alpha = X_train.drop([col for col in X_train.columns if 'alpha' in col], axis=1).duplicated().sum()\n","print(dupl/len(X_train)*100)\n","print(dupl_wo_alpha/len(X_train)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Test data\n","dupl = X_test.duplicated().sum()\n","dupl_wo_alpha = X_test.drop([col for col in X_test.columns if 'alpha' in col], axis=1).duplicated().sum()\n","print(dupl/len(X_test)*100)\n","print(dupl_wo_alpha/len(X_test)*100)"]},{"cell_type":"markdown","metadata":{},"source":["### Feature importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Using RFC attribute feature_importance_, which is based on position in decision trees (higher = more important, since it's greedy)\n","feature_names = list(num_mapper.transformed_names_) + list(cat_mapper.transformed_names_)\n","sorted(zip(rfc1.feature_importances_, feature_names), reverse=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Predictions "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def retrieve_imputed_assembled_feature_matrix(CDG_lookup, pdate):\n","    #nh_features\n","    nh_features = CDG_lookup.drop_duplicates('codigo_barrio')[['Area', 'Pop_cnt', 'median_age', 'median_income','Male_pop_ratio', 'Age18Plus_ratio']]\n","\n","    #alpha\n","    alpha_cols=[col for col in CDG_lookup if 'alpha' in col]\n","    alpha_features = CDG_lookup.drop_duplicates(['fecha','codigo_barrio','conducta']).groupby(['codigo_barrio','conducta']).median()[alpha_cols]\n","    alpha_features = alpha_features.reset_index().drop('codigo_barrio', axis=1)\n","\n","    #evictions\n","    evictions_median = CDG_lookup.drop_duplicates(['codigo_barrio','year']).median()['evictions_PkC']\n","    nh_year_features = pd.DataFrame(data=evictions_median, index=[0], columns={'evictions_PkC'})\n","\n","    #date\n","    date_features = {}\n","    pdate = pd.to_datetime(pdate)\n","    date_features['weekday_num'] = pdate.weekday()\n","    date_features['weekend'] = np.asscalar(np.where(date_features['weekday_num'] >= 4, 1, 0))\n","    date_features['Season'] = (pdate - pd.DateOffset(months=1)).quarter\n","    date_features = pd.DataFrame(data=date_features, index=[0])\n","\n","    #Crime cats\n","    crime_cats = pd.DataFrame(crime_data_grd.drop_duplicates('conducta')['conducta'])\n","\n","    #Stitch together\n","    X_pred = nh_features.reset_index(drop=True).copy()\n","    X_pred = X_pred.merge(nh_year_features, left_index=True, right_index=True)\n","    X_pred = X_pred.merge(date_features, left_index=True, right_index=True)\n","    X_pred = X_pred.merge(crime_cats, how='right', left_index=True, right_index=True).fillna(method='ffill').fillna(method='bfill')\n","    X_pred = X_pred.merge(alpha_features, on='conducta')\n","    X_pred = X_pred.reindex(sorted(X_pred.columns), axis=1)\n","\n","    return X_pred\n","\n","def get_crime_probas(codigo_barrio, pdate, model=rfc1):\n","    #make sure we have a valid date\n","    try:\n","        pd.to_datetime(pdate)\n","    except:\n","        raise ValueError('pd.to_datetime() failed, only supports valid dates.')\n","    drop_columns=['Crime_count','Age18Plus_cnt', 'evictions','fecha','codigo_barrio','geometry','year']\n","    #Slice main table\n","    CDG_lookup = crime_data_grd.query('fecha == @pdate and codigo_barrio == @codigo_barrio')\n","    #match in lookup table, construct X_pred easily\n","    if len(CDG_lookup) > 0:\n","        X_pred = CDG_lookup.drop(drop_columns, axis=1)\n","        X_pred_prep = full_pipeline.transform(X_pred)\n","        y_pred = model.predict_proba(X_pred_prep)\n","    #no match in table, impute date dependent historical features, and construct X_pred\n","    else:\n","        CDG_lookup = crime_data_grd.query('codigo_barrio == @codigo_barrio')#.drop('geometry', axis=1)\n","        #match on codigo_barrio\n","        if len(CDG_lookup) > 0:\n","            X_pred = retrieve_imputed_assembled_feature_matrix(CDG_lookup, pdate)\n","            X_pred_prep = full_pipeline.transform(X_pred)\n","            y_pred = model.predict_proba(X_pred_prep)\n","        #No match on codigo_barrio, throw error\n","        else:\n","            codigo_barrios = list(crime_data_grd.drop_duplicates('codigo_barrio')['codigo_barrio'])\n","            raise ValueError('Only supports (correctly spelled) SF neighborhoods: \\n\\n'+ str(codigo_barrios))\n","    #print neatly\n","    print(f'{codigo_barrio} on {pdate}')\n","    for cat, pred in zip(X_pred['conducta'],np.round(y_pred[:,1],2)):\n","        print(f'{cat:<20}{pred:<10}')\n","    print('\\n')\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_crime_probas('101', '28-02-2018')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNPBH1eMsEzmsT6Epr45kz9","mount_file_id":"1HMRamMbtGLEDhYu7N1LdjVfuBh7IQIZJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
