{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Install package\n","\n","Puedes instalar las dependencias listadas en el archivo `requirements.txt` utilizando el siguiente comando:\n","\n","```bash\n","pip install -r requirements.txt"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"n3J-_EWHh_A2"},"source":["## Data"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RIom_SBDhAbl"},"outputs":[{"name":"stdout","output_type":"stream","text":["-> data shape  (17707, 7)\n","-> Total de duplicados en el DataFrame: 8923\n","-> Registros duplicados que contienen valores NaN: (2472, 7)\n","-> Missing values per column:\n","fecha_hecho         0\n","fecha               0\n","year                0\n","codigo_barrio       0\n","latitud          4143\n","longitud         4143\n","conducta            0\n","dtype: int64\n","-> data shape  (7113, 7)\n","\n","crimes dtypes:  fecha_hecho      datetime64[ns]\n","fecha            datetime64[ns]\n","year                      int64\n","codigo_barrio             int64\n","latitud                 float64\n","longitud                float64\n","conducta                 object\n","dtype: object\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dayofyear</th>\n","      <th>weekofyear</th>\n","      <th>quarter</th>\n","      <th>weekday</th>\n","      <th>weekend</th>\n","      <th>fecha</th>\n","      <th>year</th>\n","      <th>codigo_barrio</th>\n","      <th>latitud</th>\n","      <th>longitud</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7113.000000</td>\n","      <td>7113.0</td>\n","      <td>7113.000000</td>\n","      <td>7113.000000</td>\n","      <td>7113.000000</td>\n","      <td>7113</td>\n","      <td>7113.000000</td>\n","      <td>7113.000000</td>\n","      <td>7113.000000</td>\n","      <td>7113.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>193.928160</td>\n","      <td>28.238577</td>\n","      <td>2.633769</td>\n","      <td>2.994095</td>\n","      <td>0.426402</td>\n","      <td>2009-02-01 11:05:26.444538368</td>\n","      <td>2008.558836</td>\n","      <td>974.879938</td>\n","      <td>6.255260</td>\n","      <td>-75.575613</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2008-01-01 00:00:00</td>\n","      <td>2008.000000</td>\n","      <td>101.000000</td>\n","      <td>6.169881</td>\n","      <td>-75.691148</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>106.000000</td>\n","      <td>16.0</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>2008-07-30 00:00:00</td>\n","      <td>2008.000000</td>\n","      <td>602.000000</td>\n","      <td>6.239632</td>\n","      <td>-75.590200</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>201.000000</td>\n","      <td>29.0</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>2009-02-10 00:00:00</td>\n","      <td>2009.000000</td>\n","      <td>1016.000000</td>\n","      <td>6.252953</td>\n","      <td>-75.571806</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>286.000000</td>\n","      <td>41.0</td>\n","      <td>4.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>2009-08-17 00:00:00</td>\n","      <td>2009.000000</td>\n","      <td>1211.000000</td>\n","      <td>6.275024</td>\n","      <td>-75.561147</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>366.000000</td>\n","      <td>53.0</td>\n","      <td>4.000000</td>\n","      <td>6.000000</td>\n","      <td>1.000000</td>\n","      <td>2009-12-31 00:00:00</td>\n","      <td>2009.000000</td>\n","      <td>9003.000000</td>\n","      <td>6.343814</td>\n","      <td>-75.514766</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>105.649092</td>\n","      <td>15.118914</td>\n","      <td>1.110144</td>\n","      <td>1.953471</td>\n","      <td>0.494588</td>\n","      <td>NaN</td>\n","      <td>0.496561</td>\n","      <td>647.807031</td>\n","      <td>0.027002</td>\n","      <td>0.020601</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         dayofyear  weekofyear      quarter      weekday      weekend  \\\n","count  7113.000000      7113.0  7113.000000  7113.000000  7113.000000   \n","mean    193.928160   28.238577     2.633769     2.994095     0.426402   \n","min       1.000000         1.0     1.000000     0.000000     0.000000   \n","25%     106.000000        16.0     2.000000     1.000000     0.000000   \n","50%     201.000000        29.0     3.000000     3.000000     0.000000   \n","75%     286.000000        41.0     4.000000     5.000000     1.000000   \n","max     366.000000        53.0     4.000000     6.000000     1.000000   \n","std     105.649092   15.118914     1.110144     1.953471     0.494588   \n","\n","                               fecha         year  codigo_barrio      latitud  \\\n","count                           7113  7113.000000    7113.000000  7113.000000   \n","mean   2009-02-01 11:05:26.444538368  2008.558836     974.879938     6.255260   \n","min              2008-01-01 00:00:00  2008.000000     101.000000     6.169881   \n","25%              2008-07-30 00:00:00  2008.000000     602.000000     6.239632   \n","50%              2009-02-10 00:00:00  2009.000000    1016.000000     6.252953   \n","75%              2009-08-17 00:00:00  2009.000000    1211.000000     6.275024   \n","max              2009-12-31 00:00:00  2009.000000    9003.000000     6.343814   \n","std                              NaN     0.496561     647.807031     0.027002   \n","\n","          longitud  \n","count  7113.000000  \n","mean    -75.575613  \n","min     -75.691148  \n","25%     -75.590200  \n","50%     -75.571806  \n","75%     -75.561147  \n","max     -75.514766  \n","std       0.020601  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import geopandas as gpd\n","from shapely.geometry import Point\n","\n","import pyproj\n","import matplotlib.pyplot as plt\n","import re\n","\n","from sklearn.preprocessing import StandardScaler, LabelBinarizer\n","from sklearn_pandas import DataFrameMapper, gen_features\n","from sklearn.pipeline import FeatureUnion\n","\n","# import shapely.geometry\n","# from sklearn.model_selection import train_test_split\n","\n","\n","def clean_conducta(conducta):\n","    conducta = conducta.upper()\n","    conducta = conducta.replace(\" \", \"_\")\n","    conducta = re.sub(\n","        r\"[ÁÉÍÓÚ]\",\n","        lambda m: m.group(0)\n","        .replace(\"Á\", \"A\")\n","        .replace(\"É\", \"E\")\n","        .replace(\"Í\", \"I\")\n","        .replace(\"Ó\", \"O\")\n","        .replace(\"Ú\", \"U\"),\n","        conducta,\n","    )\n","    conducta = conducta.replace(\"Ñ\", \"N\")\n","    return conducta\n","\n","\n","crimes_df = pd.read_csv(\"./base_crim16_08.csv\")\n","\n","# Select relevant features\n","features = [\n","    \"fecha_hecho\",\n","    \"fecha\",\n","    \"year\",\n","    \"codigo_barrio\",\n","    \"latitud\",\n","    \"longitud\",\n","    \"conducta\",\n","]\n","crimes_df = crimes_df[features]\n","\n","# Eliminar espacios al principio y al final de los valores\n","crimes_df[\"fecha\"] = crimes_df[\"fecha\"].str.strip()\n","# Convertir a datetime\n","crimes_df[\"fecha\"] = pd.to_datetime(crimes_df[\"fecha\"], format=\"%d/%m/%Y\")\n","\n","crimes_df[\"fecha_hecho\"] = pd.to_datetime(\n","    crimes_df[\"fecha_hecho\"], format=\"%d/%m/%Y %H:%M\"\n",")\n","crimes_df = crimes_df[\n","    (crimes_df[\"year\"] >= 2008) & (crimes_df[\"year\"] <= 2009)\n","]  # ? Select year 2008 and 2009\n","\n","# ?---------------------------------------------------------------------- clean data\n","print(\"-> data shape \", crimes_df.shape)\n","total_duplicados = crimes_df.duplicated().sum()\n","print(f\"-> Total de duplicados en el DataFrame: {total_duplicados}\")\n","# Identificar registros duplicados\n","duplicados = crimes_df[crimes_df.duplicated()]\n","# Verificar si hay valores NaN en los registros duplicados\n","duplicados_con_nan = duplicados[duplicados.isnull().any(axis=1)]\n","print(f\"-> Registros duplicados que contienen valores NaN: {duplicados_con_nan.shape}\")\n","\n","# Imprimir el total de valores NaN en cada columna\n","total_nan_por_columna = crimes_df.isnull().sum()\n","print(\"-> Missing values per column:\")\n","print(total_nan_por_columna)\n","\n","crimes_df = crimes_df.drop_duplicates()\n","crimes_df = crimes_df.dropna()\n","print(\"-> data shape \", crimes_df.shape)\n","# ?---------------------------------------------------------------------- clean data\n","\n","print(\"\\ncrimes dtypes: \", crimes_df.dtypes)\n","\n","# ? DATE TIME STAMP FUNCTION\n","column_1 = crimes_df.iloc[:, 0]\n","\n","db = pd.DataFrame(\n","    {\n","        \"dayofyear\": column_1.dt.dayofyear,\n","        \"weekofyear\": column_1.dt.isocalendar().week,\n","        \"quarter\": column_1.dt.quarter,\n","        \"weekday\": column_1.dt.weekday,\n","        \"weekend\": np.where(column_1.dt.weekday >= 4, 1, 0),\n","        # \"Season\": (\n","        #     column_1 - pd.DateOffset(months=1)\n","        # ).dt.quarter,\n","    }\n",")\n","\n","\n","crimes_df = crimes_df.drop(\"fecha_hecho\", axis=1)  #! drop fecha_hecho\n","crimes_df = pd.concat([db, crimes_df], axis=1)\n","crimes_df[\"conducta\"] = crimes_df[\"conducta\"].apply(clean_conducta)\n","# data = pd.get_dummies(crimes_df.conducta)\n","# crimes_df = pd.concat([data, crimes_df], axis=1)\n","# crimes_df[\"codigo_barrio\"] = crimes_df[\"codigo_barrio\"].astype(str).str.replace(\"#\", \"\")\n","\n","crimes_df[\"codigo_barrio\"] = pd.to_numeric(crimes_df[\"codigo_barrio\"], errors=\"coerce\")\n","\n","\n","crimes_df.describe()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","-> conducta unique values:  ['HURTO_DE_CARRO' 'HURTO_A_PERSONA' 'EXTORSION' 'HOMICIDIO'\n"," 'HURTO_A_RESIDENCIA']\n"]}],"source":["# imprimir los valores unicos de conducta\n","print(\"\\n-> conducta unique values: \", crimes_df[\"conducta\"].unique())"]},{"cell_type":"markdown","metadata":{},"source":["### SHP and intersections"]},{"cell_type":"markdown","metadata":{},"source":["#### Grids"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Set up transformers, EPSG:3857 is metric, same as EPSG:900913\n","# to_proxy_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","# to_original_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","\n","\n","# # Definir la función de conversión de grados a metros\n","# def convertir_lat_long_a_metros(lat, long):\n","#     transformed = to_proxy_transformer.transform(lat, long)\n","#     return transformed[0], transformed[1]\n","\n","\n","# # Perform spatial join by rounding coordinates for more accurate intersection\n","# # def round_coords(geom):\n","# #     if geom.type == \"Polygon\":\n","# #         return shapely.geometry.Polygon(\n","# #             [(round(x, 4), round(y, 4)) for x, y in geom.exterior.coords]\n","# #         )\n","# #     elif geom.type == \"Point\":\n","# #         return Point(round(geom.x, 4), round(geom.y, 4))\n","# #     else:\n","# #         return geom\n","\n","\n","# def create_gdf(df):\n","#     gdf = df.copy()\n","#     gdf[\"Coordinates\"] = list(zip(gdf.longitud, gdf.latitud))\n","#     gdf.Coordinates = gdf.Coordinates.apply(Point)\n","#     gdf = gpd.GeoDataFrame(gdf, geometry=\"Coordinates\", crs=\"epsg:32618\")\n","#     return gdf\n","\n","\n","# # Leer el archivo SHP y crear un objeto GeoDataFrame\n","# ruta_archivo_shp = \"shp/BarrioVereda_2014.shp\"\n","# #ruta_archivo_shp = \"shp/map.shp\"\n","# gdf = gpd.read_file(ruta_archivo_shp)\n","\n","# # Filtrar por el código de comunas de Medellín (son 16 comunas en total)\n","# gdf = gdf[gdf[\"LIMITECOMU\"] < \"17\"]\n","\n","# # Reproyectar a coordenadas planas\n","# gdf = gdf.to_crs(epsg=32618)\n","\n","# print(gdf.columns.tolist()) #mostar los nombres de las columnas de la base\n","\n","# # Crear el polígono único que representa el límite exterior de todos los polígonos\n","# outter_map = gdf.unary_union\n","\n","# # Crear un nuevo GeoDataFrame con el polígono del límite exterior\n","# gdf_outter = gpd.GeoDataFrame(geometry=[outter_map], crs=gdf.crs)\n","\n","# xmin, ymin, xmax, ymax = gdf_outter.total_bounds\n","# print(\"xmin:\", xmin, \"ymin:\", ymin, \"xmax:\", xmax, \"ymax:\", ymax)\n","\n","# stepsize = 200  # 200 meters grid step size\n","\n","# # Iterate over 2D area and create grid squares (boxes) with consecutive grid_id\n","# gridboxes = []\n","# grid_id = 1  # Initialize grid_id to start from 1\n","# x = xmin\n","# while x < xmax:\n","#     y = ymin\n","#     while y < ymax:\n","#         box = shapely.geometry.box(x, y, x + stepsize, y + stepsize)\n","#         box_properties = {\"geometry\": box, \"grid_id\": grid_id}\n","#         gridboxes.append(box_properties)\n","#         grid_id += 1  # Increment grid_id for the next box\n","#         y += stepsize\n","#     x += stepsize\n","\n","# # Create a GeoDataFrame from gridboxes\n","# grid_gdf = gpd.GeoDataFrame(gridboxes, crs=gdf.crs)\n","# # grid_gdf[\"geometry\"] = grid_gdf[\"geometry\"].apply(round_coords)\n","\n","# print(grid_gdf.columns.tolist()) #mostar los nombres de las columnas de la base AQUI SE MUESTRA AREA\n","\n","# # Perform spatial join\n","# grid_outter_gdf = gpd.sjoin(grid_gdf, gdf_outter, how=\"inner\", op=\"within\")\n","# # grid_outter_gdf[\"geometry\"] = grid_outter_gdf[\"geometry\"].apply(round_coords)\n","\n","# print(grid_outter_gdf.columns.tolist()) #mostar los nombres de las columnas de la base AQUI SE MUESTRA AREA\n","\n","# # **** 2. Cargar los datos de los delitos ****\n","# crimes_gdf = create_gdf(crimes_df)\n","# print(len(crimes_gdf))\n","# # crimes_gdf[\"Coordinates\"] = crimes_gdf[\"Coordinates\"].apply(round_coords)\n","\n","# # Convertir las coordenadas de latitud y longitud a metros y crear una nueva columna\n","# crimes_gdf[\"Coordinates\"] = crimes_gdf.apply(\n","#     lambda row: Point(*convertir_lat_long_a_metros(row[\"latitud\"], row[\"longitud\"])),\n","#     axis=1,\n","# )\n","\n","\n","# # eliminar todos los coordinates que tengan infinity\n","# crimes_gdf = crimes_gdf[\n","#     crimes_gdf[\"Coordinates\"].apply(lambda x: not np.isinf(x.coords[0][0]))\n","# ]\n","\n","# # eliminar todos los registros que tengan latitud y longitud 0\n","# crimes_gdf = crimes_gdf[\n","#     crimes_gdf[\"Coordinates\"].apply(\n","#         lambda x: x.coords[0][0] != 0 and x.coords[0][1] != 0\n","#     )\n","# ]\n","\n","# # Realizar la intersección usando gpd.sjoin()\n","# crimes_gdf = gpd.sjoin(\n","#     crimes_gdf, grid_outter_gdf[[\"grid_id\", \"geometry\"]], how=\"inner\", op=\"within\"\n","# )\n","# print(len(crimes_gdf))\n","\n","# # eliminar columna de index_right\n","# crimes_gdf = crimes_gdf.drop(columns=[\"index_right\"])\n","\n","# print(crimes_gdf.head())\n","\n","# print(\"\\n --------------------\")\n","\n","# print(grid_outter_gdf.head())\n","\n","# # Plot the map and grid squares\n","# fig, ax = plt.subplots(figsize=(10, 8))\n","# grid_outter_gdf.plot(ax=ax, color=\"white\", edgecolor=\"black\")\n","# crimes_gdf.plot(ax=ax, color=\"red\", markersize=1)\n","# # gdf_outter.plot(ax=ax, color='lightgray', edgecolor='black')\n","\n","\n","# plt.title(\"Mapa con cuadrícula de cuadrados de 200 metros\")\n","# plt.xlabel(\"Longitud (EPSG:32618)\")\n","# plt.ylabel(\"Latitud (EPSG:32618)\")\n","# plt.show()\n","#print(gdf.columns.tolist()) #para ver el nombre de las columnas de una base de datos"]},{"cell_type":"markdown","metadata":{},"source":["### Barrios"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["         dayofyear  weekofyear      quarter      weekday      weekend  \\\n","count  7050.000000      7050.0  7050.000000  7050.000000  7050.000000   \n","mean    193.614184   28.192624     2.630638     2.987801     0.424965   \n","min       1.000000         1.0     1.000000     0.000000     0.000000   \n","25%     106.000000        16.0     2.000000     1.000000     0.000000   \n","50%     201.000000        29.0     3.000000     3.000000     0.000000   \n","75%     286.000000        41.0     4.000000     5.000000     1.000000   \n","max     366.000000        53.0     4.000000     6.000000     1.000000   \n","std     105.626252   15.115803     1.110071     1.952658     0.494373   \n","\n","                               fecha         year  codigo_barrio      latitud  \\\n","count                           7050  7050.000000    7050.000000  7050.000000   \n","mean   2009-01-31 20:20:13.276595712  2008.558014     934.976170     6.255425   \n","min              2008-01-01 00:00:00  2008.000000     101.000000     6.177433   \n","25%              2008-07-29 00:00:00  2008.000000     601.000000     6.239760   \n","50%              2009-02-09 00:00:00  2009.000000    1016.000000     6.253009   \n","75%              2009-08-16 00:00:00  2009.000000    1209.000000     6.275053   \n","max              2009-12-31 00:00:00  2009.000000    1621.000000     6.312181   \n","std                              NaN     0.496658     421.067833     0.026750   \n","\n","          longitud          area  \n","count  7050.000000  7.050000e+03  \n","mean    -75.575340  4.421127e+05  \n","min     -75.631363  5.800020e+04  \n","25%     -75.589860  2.976083e+05  \n","50%     -75.571675  3.933844e+05  \n","75%     -75.561138  5.803761e+05  \n","max     -75.526191  1.401969e+06  \n","std       0.020176  1.992061e+05  \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nicolas\\Documents\\TRABAJOS\\TESIS_CAMILA\\workspace_python\\myvenv39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n","  if await self.run_code(code, result, async_=asy):\n"]}],"source":["# Set up transformers, EPSG:3857 is metric, same as EPSG:900913\n","to_proxy_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","to_original_transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:32618\")\n","\n","\n","# Definir la función de conversión de grados a metros\n","def convertir_lat_long_a_metros(lat, long):\n","    transformed = to_proxy_transformer.transform(lat, long)\n","    return transformed[0], transformed[1]\n","\n","\n","def create_gdf(df):\n","    gdf = df.copy()\n","    gdf[\"Coordinates\"] = list(zip(gdf.longitud, gdf.latitud))\n","    gdf.Coordinates = gdf.Coordinates.apply(Point)\n","    gdf = gpd.GeoDataFrame(gdf, geometry=\"Coordinates\", crs=\"epsg:32618\")\n","    return gdf\n","\n","\n","crimes_gdf = create_gdf(crimes_df)\n","# print(\"len crimes_gdf ->\",len(crimes_gdf))\n","\n","# Convertir las coordenadas de latitud y longitud a metros y crear una nueva columna\n","crimes_gdf[\"Coordinates\"] = crimes_gdf.apply(\n","    lambda row: Point(*convertir_lat_long_a_metros(row[\"latitud\"], row[\"longitud\"])),\n","    axis=1,\n",")\n","\n","\n","# ? eliminar todos los coordinates que tengan infinity\n","crimes_gdf = crimes_gdf[\n","    crimes_gdf[\"Coordinates\"].apply(lambda x: not np.isinf(x.coords[0][0]))\n","]\n","\n","# ? eliminar todos los registros que tengan latitud y longitud 0\n","crimes_gdf = crimes_gdf[\n","    crimes_gdf[\"Coordinates\"].apply(\n","        lambda x: x.coords[0][0] != 0 and x.coords[0][1] != 0\n","    )\n","]\n","\n","# ***************** Leer el archivo SHP y crear un objeto GeoDataFrame ***************\n","ruta_shp = \"shp/map.shp\"\n","gdf = gpd.read_file(ruta_shp)\n","\n","# Filtrar por el código de comunas de Medellín (son 16 comunas en total)\n","gdf = gdf[gdf[\"LIMITECOMU\"] < \"17\"]\n","\n","# Reproyectar a coordenadas planas\n","gdf = gdf.to_crs(epsg=32618)\n","\n","# imprimir algunas filas del archivo shp\n","# print(gdf.head(5))\n","# Realizar la intersección usando gpd.sjoin()\n","crimes_gdf = gpd.sjoin(crimes_gdf, gdf[[\"area\", \"geometry\"]], how=\"inner\", op=\"within\")\n","\n","# eliminar columna de index_right\n","crimes_gdf = crimes_gdf.drop(columns=[\"index_right\"])\n","\n","# print(\"\\n ---------------------------------------------------------- \\n\")\n","print(crimes_gdf.describe())"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>crime_count</th>\n","    </tr>\n","    <tr>\n","      <th>fecha</th>\n","      <th>codigo_barrio</th>\n","      <th>conducta</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"30\" valign=\"top\">2008-01-01</th>\n","      <th rowspan=\"5\" valign=\"top\">1001</th>\n","      <th>EXTORSION</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_RESIDENCIA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">1003</th>\n","      <th>EXTORSION</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_RESIDENCIA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">1004</th>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_RESIDENCIA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">1005</th>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_RESIDENCIA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">1006</th>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_RESIDENCIA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">1007</th>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">1008</th>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_DE_CARRO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">101</th>\n","      <th>HOMICIDIO</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>HURTO_A_PERSONA</th>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             crime_count\n","fecha      codigo_barrio conducta                       \n","2008-01-01 1001          EXTORSION                   0.0\n","                         HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_A_RESIDENCIA          0.0\n","                         HURTO_DE_CARRO              0.0\n","           1003          EXTORSION                   0.0\n","                         HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_A_RESIDENCIA          0.0\n","                         HURTO_DE_CARRO              0.0\n","           1004          HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_A_RESIDENCIA          0.0\n","                         HURTO_DE_CARRO              0.0\n","           1005          HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_A_RESIDENCIA          0.0\n","                         HURTO_DE_CARRO              0.0\n","           1006          HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_A_RESIDENCIA          0.0\n","                         HURTO_DE_CARRO              0.0\n","           1007          HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_DE_CARRO              0.0\n","           1008          HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0\n","                         HURTO_DE_CARRO              0.0\n","           101           HOMICIDIO                   0.0\n","                         HURTO_A_PERSONA             0.0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#convertir la columna de codigo de barrio a string\n","crimes_gdf[\"codigo_barrio\"] = crimes_gdf[\"codigo_barrio\"].astype(str)\n","\n","# Agrupar los datos por 'codigo_barrio', 'fecha' y 'conducta' y contar las ocurrencias\n","crime_count_per_day = crimes_gdf.groupby([\"codigo_barrio\", \"fecha\", \"conducta\"]).size()\n","\n","# Desapilar y rellenar los datos faltantes con 0\n","crime_count_per_day = (\n","    crime_count_per_day.unstack([\"codigo_barrio\", \"conducta\"])\n","    .asfreq(\"D\")\n","    .fillna(0)\n","    .stack([\"codigo_barrio\", \"conducta\"])\n",")\n","\n","crime_count_per_day = pd.DataFrame(crime_count_per_day).rename(\n","    columns={0: \"crime_count\"}\n",")\n","\n","crime_count_per_day.head(30)\n","# crime_count_per_day.to_csv(\"crimecount.csv\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(611116, 1)\n"]}],"source":["print(crime_count_per_day.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crime_count_per_day.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# crime_count_per_day = crime_count_per_day.merge(\n","#     crimes_gdf[\n","#         [\n","#             \"codigo_barrio\",\n","#             \"area\",\n","#         ]\n","#     ],\n","#     on=\"codigo_barrio\",\n","#     how=\"left\",\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Instances of crime_count per date, neighborhood, and crime category\n","cc = crime_count_per_day.groupby('crime_count').size()\n","fig, axes = plt.subplots(1, 2, figsize=(14, 4));\n","cc.plot.bar(logy=False, ax=axes[0], title='Instances of crime_count, lin axes');\n","cc.plot.bar(logy=True, ax=axes[1], title='Instances of crime_count, log-lin axes');"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crime_count_per_day.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### Load and Merge characteristics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# crime_count_per_day = crime_count_per_day.merge(\n","#     crimes_gdf[\n","#         [\n","#             \"codigo_barrio\",\n","#             \"weekday\",\n","#             \"weekend\",\n","#             \"dayofyear\",\n","#             \"weekofyear\",\n","#             \"quarter\",\n","#         ]\n","#     ],\n","#     on=\"codigo_barrio\",\n","#     how=\"left\",\n","# )\n","# crime_count_per_day.sample(5)\n","# # crime_count_per_day.to_csv('crimecount.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["characteristics = pd.read_excel(\"./characteristics.xlsx\")\n","#characteristics=characteristics[['CODIGO','year','edad','inseg','gender','atraco','seg', 'casado','pensionado', 'estra_12', 'ofi_hogar', 'acu_si','ipm','childhood','youth','adult','def_cuantitativo','def_cualitativo']]\n","characteristics=characteristics[['CODIGO','year','edad','inseg','atraco','seg','ipm','youth','adult','def_cuantitativo','def_cualitativo']]\n","\n","# eliminaar registros que tengan CODIGO null nan o con un caracter '.'\n","characteristics = characteristics[characteristics[\"CODIGO\"].notna()]\n","characteristics = characteristics[characteristics[\"CODIGO\"] != \".\"]\n","\n","# convertir a entero el codigo\n","characteristics[\"CODIGO\"] = characteristics[\"CODIGO\"].astype(int)\n","\n","# renombrar columna de characteristics \"CODIGO\" a \"codigo_barrio\"\n","characteristics = characteristics.rename(columns={\"CODIGO\": \"codigo_barrio\"})\n","\n","# Se realiza el merge de crime_count_per_day characteristics left de crime_count_per_day\n","crime_count_per_day['year'] = pd.to_datetime(crime_count_per_day['fecha']).dt.year\n","crime_count_per_day = crime_count_per_day.merge(characteristics, how='left', on=['codigo_barrio', 'year'])\n","\n","print(\"-> Missing values: \\n\")\n","print(crime_count_per_day.isnull().sum())\n","# drop missing values\n","print(\"-> Dropping missing values: \\n\")\n","crime_count_per_day = crime_count_per_day.dropna()\n","# elimianr los datos repetidos\n","print(\"-> Eliminar los datos repetidos \\n\")\n","crime_count_per_day = crime_count_per_day.drop_duplicates()\n","\n","crime_count_per_day.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Description of our new dataset \n","crime_count_per_day.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crime_count_per_day.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save crime count per day into csv file\n","# crime_count_per_day.to_csv(\"crime_count_per_day.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["crime_count_per_day.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### Alpha"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["alpha = 7\n","aslice = pd.DataFrame()\n","for cond in crime_count_per_day[\"conducta\"].unique():\n","    for cod in crime_count_per_day[\"codigo_barrio\"].unique():\n","        # Slice data per cod-conducta\n","        dslice = crime_count_per_day.query(\n","            \"conducta == @cond and codigo_barrio == @cod\"\n","        )[[\"codigo_barrio\", \"conducta\", \"fecha\", \"crime_count\"]]\n","        # Rolling sum, don't allow starting nan:s\n","        rollslice = (\n","            dslice[[\"fecha\", \"crime_count\"]]\n","            .rolling(alpha, center=False, on=\"fecha\")\n","            .sum()\n","            .fillna(method=\"bfill\")\n","        )\n","        rollslice = rollslice.rename(columns={\"crime_count\": \"Rolling_sum\"})\n","        # merge\n","        dslice = dslice.merge(rollslice, on=\"fecha\", how=\"left\").drop(\n","            \"crime_count\", axis=1\n","        )\n","        aslice = pd.concat([aslice, dslice], ignore_index=True)\n","# merge with main table\n","crime_count_per_day = crime_count_per_day.merge(\n","    aslice, on=[\"fecha\", \"codigo_barrio\", \"conducta\"], how=\"left\"\n",")\n","colnames = [\"alpha_crime_dens_area_\" + str(alpha)]\n","\n","# normalize\n","crime_count_per_day[colnames[0]] = (\n","    crime_count_per_day[\"Rolling_sum\"] / crime_count_per_day[\"area\"]\n",")\n","crime_count_per_day = crime_count_per_day.drop(\"Rolling_sum\", axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["alpha = 30\n","aslice = pd.DataFrame()\n","\n","for cond in crime_count_per_day[\"conducta\"].unique():\n","    for cod in crime_count_per_day[\"codigo_barrio\"].unique():\n","        dslice = crime_count_per_day.query(\n","            \"conducta == @cond and codigo_barrio == @cod\"\n","        )[[\"codigo_barrio\", \"conducta\", \"fecha\", \"crime_count\"]]\n","        rollslice = (\n","            dslice[[\"fecha\", \"crime_count\"]]\n","            .rolling(alpha, center=False, on=\"fecha\")\n","            .sum()\n","            .fillna(method=\"bfill\")\n","        )\n","        rollslice = rollslice.rename(columns={\"crime_count\": \"Rolling_sum\"})\n","        dslice = dslice.merge(rollslice, on=\"fecha\", how=\"left\").drop(\n","            \"crime_count\", axis=1\n","        )\n","        aslice = pd.concat([aslice, dslice], ignore_index=True)\n","\n","crime_count_per_day = crime_count_per_day.merge(\n","    aslice, on=[\"fecha\", \"codigo_barrio\", \"conducta\"], how=\"left\"\n",")\n","\n","colnames = [\"alpha_crime_dens_area_\" + str(alpha)]\n","crime_count_per_day[colnames[0]] = (\n","    crime_count_per_day[\"Rolling_sum\"] / crime_count_per_day[\"area\"]\n",")\n","crime_count_per_day = crime_count_per_day.drop(\"Rolling_sum\", axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### Correlation features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import seaborn as sns\n","# sns.set()\n","# crime_count_per_day_cd = crime_count_per_day.drop(['conducta','crime_count'], axis=1)\n","\n","# #Adding dummy feature to get full cmap scale (really complicated to solve in sns/matplotlib)\n","# crime_count_per_day_cd['dummy_feature'] = -crime_count_per_day_cd['pensionado']\n","# corr = crime_count_per_day_cd.corr()\n","\n","# # Create a heatmap\n","# fig, ax = plt.subplots(figsize=(10, 8))\n","# sns.heatmap(corr,\n","#             xticklabels=corr.columns.values,\n","#             yticklabels=corr.columns.values,\n","#             cmap='RdBu_r',\n","#             ax=ax)\n","\n","# # Remove the dummy feature\n","# crime_count_per_day_cd = crime_count_per_day_cd.drop('dummy_feature', axis=1)\n","\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Train-dev-test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort columns on name for easier handling of predictions downstream\n","crime_data_grd = crime_count_per_day.reindex(\n","    sorted(crime_count_per_day.columns), axis=1\n",")\n","\n","\n","X_train = crime_data_grd[crime_data_grd['year'] <= 2008] \n","#crime_data_grd.query('fecha < \"2018-01-01\"')\n","X_test = crime_data_grd[crime_data_grd['year'] == 2009]  \n","#crime_data_grd.query('fecha >= \"2018-01-01\"')\n","\n","#labels for classifier\n","y_train_clf = X_train['crime_count'].clip(upper=1).astype(int)\n","y_test_clf = X_test['crime_count'].clip(upper=1).astype(int)\n","\n","#labels for regression\n","y_train_reg = X_train['crime_count']\n","y_test_reg = X_test['crime_count']\n","\n","#Clean\n","X_train = X_train.drop(['crime_count','fecha','year','codigo_barrio'],axis=1)\n","X_test = X_test.drop(['crime_count','fecha','year','codigo_barrio'],axis=1)\n","\n","#Check split ratios\n","time_split = int(len(X_train)/(len(crime_data_grd))*100)\n","event_split = int(y_train_clf.sum()/(y_train_clf.sum()+y_test_clf.sum())*100)\n","print(\"Train/test split in time: {}/{}\".format(time_split, 100-time_split))\n","print(\"Train/test split in events: {}/{}\".format(event_split, 100-event_split))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.sample(10)"]},{"cell_type":"markdown","metadata":{},"source":["### Model preprocessing pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#source: https://github.com/ageron/handson-ml\n","class LabelBinarizerPipelineFriendly(LabelBinarizer):\n","    \"\"\"Binarize Label in a One vs all fashion. This utility class would\n","    enable people to use this with categorical variables in pipelines.\n","    Refer to LabelBinarizer for detailed info.\n","    \"\"\"\n","\n","    def fit(self, X, y=None):\n","        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n","        super(LabelBinarizerPipelineFriendly, self).fit(X)\n","\n","    def transform(self, X, y=None):\n","        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n","\n","    def fit_transform(self, X, y=None):\n","        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Numerical features. Using a StandardScaler to be insensitive to any outliers.\n","num_attributes = X_train.select_dtypes(exclude=[object]).columns\n","num_mapper = DataFrameMapper(gen_features(\n","    columns=[[f] for f in num_attributes],\n","    classes=[\n","            StandardScaler\n","            ]\n","))\n","\n","#Categorical features. One-hot encode (binarize) the features.\n","cat_attributes = X_train.select_dtypes(include=[object]).columns\n","cat_mapper = DataFrameMapper(gen_features(\n","    columns=[[f] for f in cat_attributes],\n","    classes=[LabelBinarizerPipelineFriendly]\n","))\n","\n","#Union\n","full_pipeline = FeatureUnion(transformer_list=[\n","    ('num_mapper', num_mapper),\n","    ('cat_mapper', cat_mapper),\n","])\n","\n","X_train_prep = full_pipeline.fit_transform(X_train)\n","X_test_prep = full_pipeline.transform(X_test)\n","print(num_attributes)\n","print(cat_attributes)\n","print(np.shape(X_train_prep))\n","print(X_train_prep[1])\n"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter grid search"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from scipy.stats import randint as sp_randint\n","# from sklearn.metrics import precision_recall_curve, confusion_matrix, make_scorer, classification_report\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.model_selection import cross_val_predict, RandomizedSearchCV\n","# def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n","# def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n","# def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n","# def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n","# def get_standard_scores():\n","#     scores = {'recall' : 'recall', 'precision' : 'precision',\n","#     'roc_auc' : 'roc_auc', 'avg_precision' : 'average_precision',\n","#     'f1' : 'f1', \n","#     'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n","#     'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n","#     return scores\n","# #Uncomment the three bottom lines in the cell and it'll search and print the results.\n","# param_grid = {\n","#     'max_depth': sp_randint(5, 80),\n","#     'n_estimators' : sp_randint(1, 200),\n","#     'min_samples_leaf' : sp_randint(1, 400),\n","#     'random_state' : [42],\n","#     'max_features' : ['auto'],\n","#     'class_weight' : ['balanced', None]\n","#     }\n","# def generate_results_df(grid_searcher, scores, generate_files=False):\n","#     #keep all columns for future analyses\n","#     result_df_full = pd.DataFrame(grid_searcher.cv_results_)\n","#     #create a nimbler df\n","#     result_df_compact = pd.DataFrame(index=result_df_full.index)\n","#     for col in [col for col in result_df_full.columns if 'mean_test' in col]:\n","#         #First, check to see if we have a mean_test_score column, whcih indicates only one unnamed metric (from BayesSearchCV)\n","#         #If so, fill in name; else, handle multiple metrics. We don't have to break the loop, since mean_test_score implies only one 'mean_test'-column.\n","#         if 'mean_test_score' in col and type(scores) is str:\n","#             result_df_compact[scores] = result_df_full[[col, col.replace('mean_', 'std_')]].round(3).astype(str).apply(lambda x: ' +/- '.join(x), axis=1)\n","#         else:\n","#             #Confusion matrix must be handled separately\n","#             if '_tp' not in col and '_tn' not in col and '_fn' not in col and '_fp' not in col:\n","#                 result_df_compact[col.replace('mean_test_','')] = result_df_full[[col, col.replace('mean_', 'std_')]].round(3).astype(str).apply(lambda x: ' +/- '.join(x), axis=1)\n","#             else:\n","#                 #only do the confusion once\n","#                 if 'tp_fp_fn_tn' not in result_df_compact.columns:\n","#                     for metric in ['tp','fp','fn','tn']:\n","#                         #sum confusion matrix elements to match total number of samples\n","#                         result_df_full[metric+'_sum'] = result_df_full[[column for column in result_df_full.columns if metric in column and 'split' in column]].sum(axis=1)\n","#                     result_df_compact['tp_fp_fn_tn'] = result_df_full[['tp_sum', 'fp_sum', 'fn_sum', 'tn_sum']].astype(int).astype(str).apply(lambda x: '/'.join(x), axis=1)\n","#     result_df_compact['time_fit_score'] = result_df_full[['mean_fit_time', 'mean_score_time']].round(3).astype(str).apply(lambda x: ' + '.join(x), axis=1)\n","#     #add all parameters individually to the compact df\n","#     param_cols=[col for col in result_df_full.columns if 'param_' in col]\n","#     #transfer all parameter columns to compact df\n","#     for col in param_cols:\n","#         result_df_compact[col] = result_df_full[col].copy()\n","#     #we usually want to save the results in a file\n","#     if generate_files:\n","#         estimator=str(grid_searcher.estimator).split('(')[0]+'_'\n","#         #If we use a pipeline, list all estimators/samplers/tranformers in the file name\n","#         if estimator=='Pipeline_':\n","#             estimator=\"\"\n","#             for key, value in grid_searcher.estimator.named_steps.items():\n","#                 estimator+=str(value).split('(')[0]+'_'\n","#         #TODO: set output dir of file\n","#         opwd = './'\n","#         ofile = 'Results_df_'+estimator+time.strftime(\"%Y%m%d_%H%M%S\")+'.csv'\n","#         result_df_compact.to_csv(opwd+ofile, sep=',')\n","#         ofile = 'Full_results_df_'+estimator+'_'+time.strftime(\"%Y%m%d_%H%M%S\")+'.csv'\n","#         result_df_full.to_csv(opwd+ofile, sep=',')\n","#     return result_df_full, result_df_compact\n","\n","# scores = get_standard_scores()\n","\n","# n_iter = 10; cv=3\n","# rfc = RandomForestClassifier()\n","# rnd_search = RandomizedSearchCV(rfc, param_grid, n_iter=n_iter, cv=cv, scoring=scores, n_jobs=-1, refit=False, return_train_score=False, verbose=2)\n","# rnd_search.fit(X_train_prep, y_train_clf)\n","\n","# _, gs_df = generate_results_df(rnd_search, scores)\n","# gs_df.sort_values(by='avg_precision', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import randint as sp_randint\n","from sklearn.metrics import precision_recall_curve, confusion_matrix, make_scorer, classification_report\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_predict, RandomizedSearchCV\n","\n","#Select best models: best balanced classes and unbalanced classes. \n","#Could use refit=True in search above if you want to include search and run code top-to-bottom.\n","rfc1 = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=None, max_depth=77, n_estimators=81, min_samples_leaf=152)\n","rfc1.fit(X_train_prep, y_train_clf)\n","#rfc2 = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced', max_depth=69, n_estimators=64, min_samples_leaf=132)\n","#rfc2.fit(X_train_prep, y_train_clf)"]},{"cell_type":"markdown","metadata":{},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def obtain_and_plot_evaluation_figures(model, X, y, method, datatype, cv=3, verbose=100, title_prefix=None):\n","    if datatype == 'train':\n","        y_pred = cross_val_predict(model, X, y, cv=cv, method=method, verbose=verbose)\n","    elif datatype == 'test':\n","        y_pred = model.predict_proba(X)\n","    precisions, recalls, thresholds = precision_recall_curve(y, y_pred[:,1])\n","        #y_pred = model.predict_proba(X)\n","        \n","    # if datatype == 'train':\n","    #     precisions, recalls, thresholds = precision_recall_curve(y, y_pred)\n","    # else:\n","    #     precisions, recalls, thresholds = precision_recall_curve(y, y_pred)\n","\n","    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5));\n","    ax1.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\");\n","    ax1.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\");\n","    ax1.set(xlabel=\"Thresholds\");\n","    ax1.set_ylim([0,1]);\n","    ax1.legend();\n","    ax1.set_title(title_prefix + 'Precision & recall at thresholds');\n","\n","    ax2.plot(precisions, recalls, label=None);\n","    ax2.set_ylim([0,1]);\n","    ax2.set_xlim([0,1]);\n","    ax2.set(xlabel=\"Precision\", ylabel=\"Recall\");\n","    ax2.set_title(title_prefix + 'Precision-recall curve');\n","    print(np.shape(y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pip install joblib\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import joblib\n","# Guardar el modelo en un archivo con compresión\n","model_filename = 'random_forest_model_compre_27_08.joblib'\n","joblib.dump(rfc1, model_filename, compress=5)  # El valor 3 controla el nivel de compresión\n","# Cargar el modelo desde el archivo\n","loaded_model = joblib.load(model_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(y_train_clf.head())\n","\n","print(y_train_clf.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["obtain_and_plot_evaluation_figures(model=rfc1, X=X_train_prep, y=y_train_clf, datatype='train', method='predict_proba', cv=3, verbose=0, title_prefix='CV:ed TRAINING DATA: ')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["obtain_and_plot_evaluation_figures(model=rfc1, X=X_test_prep, y=y_test_clf, datatype='test', method='predict_proba', verbose=0, title_prefix='TEST DATA: ')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('CV:ed TRAINING DATA: \\n' + classification_report(y_train_clf, cross_val_predict(rfc1, X_train_prep, y_train_clf, cv=3)))\n","print('TEST DATA: \\n' + classification_report(y_test_clf, rfc1.predict(X_test_prep)))"]},{"cell_type":"markdown","metadata":{},"source":["### Duplicated train/test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#All duplicates and also without alpha-days crime density columns\n","#Training data\n","dupl = X_train.duplicated().sum()\n","dupl_wo_alpha = X_train.drop([col for col in X_train.columns if 'alpha' in col], axis=1).duplicated().sum()\n","print(dupl/len(X_train)*100)\n","print(dupl_wo_alpha/len(X_train)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Test data\n","dupl = X_test.duplicated().sum()\n","dupl_wo_alpha = X_test.drop([col for col in X_test.columns if 'alpha' in col], axis=1).duplicated().sum()\n","print(dupl/len(X_test)*100)\n","print(dupl_wo_alpha/len(X_test)*100)"]},{"cell_type":"markdown","metadata":{},"source":["### Feature importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Using RFC attribute feature_importance_, which is based on position in decision trees (higher = more important, since it's greedy)\n","feature_names = list(num_mapper.transformed_names_) + list(cat_mapper.transformed_names_)\n","sorted(zip(rfc1.feature_importances_, feature_names), reverse=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Predictions "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def retrieve_imputed_assembled_feature_matrix(CDG_lookup, pdate):    \n","    # nh_features = CDG_lookup.drop_duplicates('codigo_barrio')[['area', 'Pop_cnt', 'median_age', 'median_income','Male_pop_ratio', 'Age18Plus_ratio']]\n","    nh_features = CDG_lookup.drop_duplicates(\"codigo_barrio\")[\n","        [\n","            \"area\",\n","            \"edad\",\n","            \"inseg\",\n","            \"adult\",\n","            \"youth\",\n","            \"ipm\",\n","            \"seg\",\n","            \"atraco\",\n","            \"def_cualitativo\",\n","            \"def_cuantitativo\",\n","        ]\n","    ]\n","\n","    #? alpha\n","    alpha_cols = [col for col in CDG_lookup if \"alpha\" in col]\n","    # alpha_features = CDG_lookup.drop_duplicates(['fecha','codigo_barrio','conducta']).groupby(['codigo_barrio','conducta']).median()[alpha_cols]\n","    alpha_features = CDG_lookup.groupby([\"codigo_barrio\", \"conducta\"])[\n","        alpha_cols\n","    ].median()\n","    alpha_features = alpha_features.reset_index().drop(\"codigo_barrio\", axis=1)\n","\n","    #? date\n","    date_features = {}\n","    pdate = pd.to_datetime(pdate, format=\"%Y-%m-%d\")\n","    date_features[\"weekday\"] = pdate.weekday()\n","    # date_features['weekend'] = np.asscalar(np.where(date_features['weekday_num'] >= 4, 1, 0))\n","    date_features[\"weekend\"] = np.where(date_features[\"weekday\"] >= 4, 1, 0)\n","    date_features = pd.DataFrame(data=date_features, index=[0])\n","\n","    #? Crime cats\n","    crime_cats = pd.DataFrame(crime_data_grd.drop_duplicates(\"conducta\")[\"conducta\"])\n","\n","    #? Stitch together\n","    X_pred = nh_features.reset_index(drop=True).copy()\n","    # X_pred = X_pred.merge(nh_year_features, left_index=True, right_index=True)\n","    X_pred = X_pred.merge(date_features, left_index=True, right_index=True)\n","    X_pred = (\n","        X_pred.merge(crime_cats, how=\"right\", left_index=True, right_index=True)\n","        .fillna(method=\"ffill\")\n","        .fillna(method=\"bfill\")\n","    )\n","    X_pred = X_pred.merge(alpha_features, on=\"conducta\")\n","    X_pred = X_pred.reindex(sorted(X_pred.columns), axis=1)\n","\n","    return X_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_crime_probas(codigo_barrio, pdate, model=rfc1):\n","    # make sure we have a valid date\n","    try:\n","        pd.to_datetime(pdate, format=\"%Y-%m-%d\")\n","    except:\n","        raise ValueError(\"pd.to_datetime() failed, only supports valid dates.\")\n","    drop_columns = [\n","        \"crime_count\",\n","        \"fecha\",\n","        \"codigo_barrio\",\n","        \"year\",\n","    ]  #\"edad\", \"geometry\"\n","    # Slice main table\n","    # CDG_lookup = crime_data_grd.query('fecha == @pdate and codigo_barrio == @codigo_barrio')\n","    CDG_lookup = crime_data_grd.query(\n","        \"fecha == @pdate and codigo_barrio == \" + str(codigo_barrio)\n","    )\n","    # print(\"CDG_lookup ->\", CDG_lookup)\n","    # match in lookup table, construct X_pred easily\n","    if len(CDG_lookup) > 0:\n","        X_pred = CDG_lookup.drop(drop_columns, axis=1)\n","        X_pred_prep = full_pipeline.transform(X_pred)\n","        y_pred = model.predict_proba(X_pred_prep)\n","    # no match in table, impute date dependent historical features, and construct X_pred\n","    else:\n","        # CDG_lookup = crime_data_grd.query('codigo_barrio == @codigo_barrio')#.drop('geometry', axis=1)\n","        CDG_lookup = crime_data_grd.query(\"codigo_barrio == \" + str(codigo_barrio))\n","        # print(\"CDG_lookup ->\", CDG_lookup)\n","        # match on codigo_barrio\n","        if len(CDG_lookup) > 0:\n","            X_pred = retrieve_imputed_assembled_feature_matrix(CDG_lookup, pdate)\n","            X_pred_prep = full_pipeline.transform(X_pred)\n","            y_pred = model.predict_proba(X_pred_prep)\n","        # No match on codigo_barrio, throw error\n","        else:\n","            codigo_barrios = list(\n","                crime_data_grd.drop_duplicates(\"codigo_barrio\")[\"codigo_barrio\"]\n","            )\n","            raise ValueError(\n","                \"Only supports (correctly spelled) SF neighborhoods: \\n\\n\"\n","                + str(codigo_barrios)\n","            )\n","    # print neatly\n","    print(f\"{codigo_barrio} on {pdate}\")\n","    for cat, pred in zip(X_pred[\"conducta\"], np.round(y_pred[:, 1], 2)):\n","        print(f\"{cat:<20}{pred:<10}\")\n","    print(\"\\n\")\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_crime_probas('101', '2009-07-05')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["codigo_barrio = '101'\n","pdate = '2009-07-05'\n","# make sure we have a valid date\n","try:\n","    pd.to_datetime(pdate, format=\"%Y-%m-%d\")\n","except:\n","    raise ValueError(\"pd.to_datetime() failed, only supports valid dates.\")\n","result_queries = crime_data_grd.query(\"fecha == @pdate and codigo_barrio == \" + str(codigo_barrio))  \n","print(result_queries) "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNPBH1eMsEzmsT6Epr45kz9","mount_file_id":"1HMRamMbtGLEDhYu7N1LdjVfuBh7IQIZJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
